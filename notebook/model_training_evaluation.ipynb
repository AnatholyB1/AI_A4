{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA for HumanForYou - Entraînement et Évaluation du Modèle\n",
    "\n",
    "|Auteur|\n",
    "|---|\n",
    "|G. DUBOYS DE LAVIGERIE|\n",
    "|T. VILLETTE|\n",
    "|O. BOUSSARD|\n",
    "|A. BRICON|\n",
    "\n",
    "## Objectifs du Notebook\n",
    "\n",
    "Ce notebook a pour objectif de développer plusieurs modèles d'IA en utilisant les données prétraitées et de les évaluer sur un ensemble de test. Les étapes comprennent :\n",
    "\n",
    "1. **Division des données :** Séparation des données en ensembles d'entraînement et de test.\n",
    "2. **Entraînement des modèles :** Développement de plusieurs modèles d'IA avec les données d'entraînement.\n",
    "3. **Évaluation des modèles :** Mesure des performances de chaque modèle sur l'ensemble de test.\n",
    "4. **Sélection du meilleur modèle :** Analyse des résultats pour choisir le modèle offrant les performances les plus prometteuses.\n",
    "\n",
    "## Attendus\n",
    "\n",
    "À la fin de ce notebook, le modèle sera prêt à être déployé, accompagné d'une évaluation détaillée de ses performances.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir exécuté le notebook `data_preprocessing.ipynb` pour garantir que les données sont prêtes pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilité entre Python 2 et Python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Imports nécessaires\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "# Assurer la stabilité du notebook entre différentes exécutions\n",
    "np.random.seed(44)\n",
    "\n",
    "# Ajustements des paramètres de l'affichage des figures dans le notebook\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Ignorer les avertissements inutiles (voir SciPy issue #5998)\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données traitées réalisée dans le notebook `data_preprocessing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "# Définit une fonction pour charger des données prétraitées\n",
    "def load_preprocessed_data(file_path='../datasets/data_preprocessing.csv'):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Appelle la fonction et stocke le résultat\n",
    "result = load_preprocessed_data()\n",
    "\n",
    "# Affiche les cinq premières lignes du DataFrame\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division des données (jeu d'entraînement, jeu de test)\n",
    "Cette étape consiste à séparer les données en un jeu d'entraînement (80%) et un jeu de test (20%) afin d'évaluer la capacité du modèle à généraliser sur de nouvelles données. \n",
    "\n",
    "- Le **jeu d'entraînement** est utilisé pour former le modèle, lui fournissant une quantité suffisante de données pour apprendre les motifs et les relations présents dans nos données.\n",
    "\n",
    "- Le **jeu de test** permet de mesurer la performance du modèle sur des données non vues pendant l'entraînement, offrant ainsi une évaluation impartiale de sa capacité de généralisation.\n",
    "\n",
    "Le choix de la division 80/20 n'est pas arbitraire. Il s'agit d'une règle bien établie dans le domaine de l'apprentissage machine, offrant un équilibre optimal entre la taille de l'ensemble d'entraînement nécessaire pour un apprentissage efficace et la capacité à évaluer le modèle de manière significative sur le jeu de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la fonction train_test_split de scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extraction de la variable cible 'Attrition' que l'on cherche à prédire\n",
    "y = result['Attrition']\n",
    "\n",
    "# Extraction des caractéristiques (paramètres) en excluant la variable cible 'Attrition'\n",
    "X = result.drop(['Attrition'], axis=1)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "# - X_train : ensemble d'entraînement des caractéristiques\n",
    "# - X_test : ensemble de test des caractéristiques\n",
    "# - y_train : ensemble d'entraînement des étiquettes (Attrition)\n",
    "# - y_test : ensemble de test des étiquettes (Attrition)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de la Répartition d'Attrition dans les Jeux d'Entraînement et de Test\n",
    "\n",
    "Le code ci-dessous utilise des diagrammes en barres pour visualiser la répartition de la variable cible \"Attrition\" dans les ensembles d'entraînement et de test. Cette représentation graphique offre une compréhension visuelle de la distribution des employés ayant quitté (Attrition) par rapport à ceux qui sont restés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de données dans chaque ensemble\n",
    "print(f\"Nombre de données dans l'ensemble d'entraînement : {len(X_train)}\")\n",
    "print(f\"Nombre de données dans l'ensemble de test : {len(X_test)}\")\n",
    "\n",
    "# Diagramme pour le jeu d'entraînement\n",
    "plt.figure(figsize=(5, 3))\n",
    "y_train.value_counts().plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Jeu d\\'entraînement')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Nombre d\\'employés')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Diagramme pour le jeu de test\n",
    "plt.figure(figsize=(5, 3))\n",
    "y_test.value_counts().plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Jeu de test')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Nombre d\\'employés')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut observer que nos ensembles d'entraînement et de test sont correctement répartis, présentant une proportion équivalente pour l'attribut \"Yes\" (2.28) et \"No\" (-0.43). Avec un total de 3528 données dans l'ensemble d'entraînement et 882 données dans l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mélange de données\n",
    "\n",
    "Nous avons effectué un mélange aléatoire des données de l'ensemble d'entraînement (`X_train` et `y_train`) en utilisant la permutation aléatoire (`np.random.permutation`). Cette étape est cruciale pour éviter les biais potentiels liés à l'ordre des données, améliorer la généralisation du modèle, et optimiser la convergence de l'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les indices avant le mélange\n",
    "indices_avant_melange = np.arange(len(X_train))\n",
    "\n",
    "# Mélanger les indices\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train.iloc[shuffle_index], y_train.iloc[shuffle_index]\n",
    "\n",
    "# Sauvegarder les indices après le mélange\n",
    "indices_apres_melange = shuffle_index\n",
    "\n",
    "# Afficher les indices avant et après le mélange\n",
    "print(\"Indices avant le mélange :\", indices_avant_melange)\n",
    "print(\"Indices après le mélange :\", indices_apres_melange)\n",
    "\n",
    "# Garder les données avec attrition à True (> 0)\n",
    "y_train_yes = (y_train > 0)\n",
    "y_test_yes = (y_test > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir et Entraîner un modèle\n",
    "\n",
    "Avant de plonger dans les détails, il est essentiel de comprendre comment on a choisi, entraîné, et évalué notre modèle. Une étape cruciale a consisté à analyser les performances de plusieurs modèles sur un ensemble de test.\n",
    "\n",
    "On a opté pour la classe de modèle dédiée à la classification plutôt que des modèles de régression. Cette décision découle de notre objectif : évaluer les caractéristiques qui influent sur le départ ou le maintien d'une personne dans l'entreprise. Étant donné la nature binaire de ce problème (départ ou non), on a jugé qu'**un modèle de classification** serait plus adapté. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'Entraînement et d'Évaluation des Modèles\n",
    "\n",
    "Nous avons décidé de créer une fonction `display_scores` qui permet d'entraîner, évaluer et afficher les scores de différents modèles de manière cohérente. Cette fonction, définie dans le code ci-dessous, a plusieurs objectifs cruciaux dans notre projet :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphiques Informatifs \n",
    "\n",
    "- La fonction génère des graphiques informatifs, tels que la courbe précision vs rappel et la courbe précision vs rappel en fonction du seuil.\n",
    "- Ces graphiques facilitent la visualisation des performances du modèle et aident à choisir un seuil adapté en fonction des besoins spécifiques de notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche le graphique de la précision et recall en fonction du seuil\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Affiche le graphique de la précision en fonction du recall\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recalls, precisions, \"k-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Affiche la matrice de confusion\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Prédiction Vraie\", \"Prédiction Fausse\"],\n",
    "                yticklabels=[\"Réalité Vraie\", \"Réalité Fausse\"])\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Réalité\")\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement et Évaluation\n",
    "\n",
    "- La fonction utilise la méthode spécifiée (decision_function ou predict_proba) pour obtenir les scores des modèles.\n",
    "- Elle entraîne le modèle sur les données fournies, évaluant ainsi ses performances à l'aide de différentes métriques telles que la matrice de confusion, la précision, le recall, le F1-score, et l'accuracy.\n",
    "- La validation croisée avec 5 plis (cv=5) est intégrée dans le processus, assurant une évaluation robuste et évitant les biais liés à une seule division des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Entraîner et évaluer le modèle puis afficher les scores\n",
    "def display_scores(model, x, y, method):\n",
    "    model.fit(x, y) # Entraîner le modèle\n",
    "\n",
    "    y_train_pred = cross_val_predict(model, x, y, cv=5) # Validation croisée\n",
    "    y_scores = cross_val_predict(model, x, y, cv=5, method=method) # Calcul des scores\n",
    "\n",
    "    # Calcul des différents indicateurs de performance\n",
    "    conf_matrix = confusion_matrix(y, y_train_pred)\n",
    "    precision = precision_score(y, y_train_pred)\n",
    "    recall = recall_score(y, y_train_pred)\n",
    "    f1 = f1_score(y, y_train_pred)\n",
    "    accuracy = accuracy_score(y, y_train_pred)\n",
    "    auc_score = roc_auc_score(y, y_train_pred)\n",
    "\n",
    "\n",
    "    # Si method est \"decision_function\", y_scores est un tableau 1D\n",
    "    if method == \"decision_function\":\n",
    "        # calculer précisions, recalls et seuils pour la courbe ROC\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "        a, b, c = roc_curve(y, y_scores)\n",
    "    # Si method est \"predict_proba\", y_scores est un tableau 2D\n",
    "    else:\n",
    "        # calculer précisions, recalls et seuils pour la courbe ROC\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y, y_scores[:, -1])\n",
    "        a, b, c = roc_curve(y, y_scores[:, -1])\n",
    "\n",
    "    \n",
    "\n",
    "    # Affichage des indicateurs de performance\n",
    "    print('Précision : ', precision)\n",
    "    print('Recall : ', recall)\n",
    "    print('F1-score : ', f1)\n",
    "    print('Accuracy : ', accuracy)\n",
    "\n",
    "    # Affichage des graphiques\n",
    "    plot_confusion_matrix(conf_matrix)\n",
    "    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "    plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "    # retourner précisions, recalls et seuils pour la courbe ROC\n",
    "    return a, b, c, auc_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle :  SDG Classifier\n",
    "\n",
    "On a choisi de tester le SDG Classifier en raison de sa capacité à converger rapidement sur des ensembles de données volumineux, ce qui s'aligne parfaitement avec notre objectif de prédire le départ ou le maintien d'une personne dans l'entreprise. Sa flexibilité quant à la fonction de perte en fait un choix pertinent pour notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Créer un SGDClassifier\n",
    "sgd_clf = SGDClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_sdg, tpr_sdg, thresholds_sdg, roc_auc_sdg = display_scores(sgd_clf, X_train, y_train_yes, \"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après nos résultats, le modèle SDGClassifier a une précision relativement faible et un recall encore plus bas, ce qui se traduit par un F1-score très faible également, indiquant une performance modérée dans l'équilibre entre précision et recall. La matrice de confusion confirme ces résultats : beaucoup de prédictions de faux positifs entraîne un ratio de positifs faible et donc un recall très bas. Ensuite, le premier graphique nous montre que le seuil optimal est d'environ -0.7 pour un recall et une précision le plus élevé possible mais qui reste toujours faible (inférieur à 0.4). Le second graphique nous montre également une précision et un recall toujours trop bas. D'après nos résultats, le SDGClassifier ne semble pas être un modèle adapté à notre situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle : RandomForestClassifier\n",
    "\n",
    "On a opté pour le RandomForestClassifier pour sa capacité à traiter des ensembles de données complexes grâce à l'agrégation d'arbres de décision. Cette approche permet de saisir des relations non linéaires et des interactions complexes entre les caractéristiques, fournissant ainsi une représentation plus complète du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créer un RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_clf, tpr_clf, thresholds_clf, roc_auc_clf = display_scores(forest_clf, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après ces résultats, la RandomForest semble être un modèle bien plus performant. En effet, la précision est très haute car le modèle a un nombre de faux positifs très bas. Le recall est également haut avec peu de faux négatifs. Le premier graphique nous montre un seuil idéal autour de 0.43 pour une précision et un recall très haut (plus de 0.9). Le second graphique nous affiche les même résultats, un recall et précision maximums a plus de 0.9. Avec son accuracy très élevée, la RandomForest semble être un modèle très bien adapté à notre problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle : Régression logistique\n",
    "\n",
    "La régression logistique a été choisie en raison de sa simplicité et de son interprétabilité. Souvent utilisée comme point de référence, elle offre une comparaison avec des modèles plus complexes, nous permettant de déterminer si sa simplicité est suffisante pour notre problème de prédiction du départ d'un employé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Créer une LogistiqueRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_lgr, tpr_lgr, thresholds_lgr, roc_auc_lgr = display_scores(logisticRegr, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après ces résultats, la régression linéaire a une précision moyenne et un recall très extrêmement bas. Les deux graphiques nous montrent un compromis maximum de ces 2 valeurs inférieur à 0,5. Ce modèle ne semble pas être assez performant pour notre situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle : VotingClassifier\n",
    "\n",
    "On a décidé d'intégrer le VotingClassifier pour exploiter les avantages de plusieurs modèles. En agrégeant les prédictions, ce modèle améliore la performance globale et réduit le risque de surajustement. Cette approche profite de la diversité des modèles individuels, offrant ainsi une robustesse accrue et une meilleure capacité de généralisation aux nouvelles données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  VotingClassifier\n",
    "\n",
    "# Créer un RandomForestClassifier et un LogisticRegression\n",
    "voting_forest = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "voting_logisticRegr = LogisticRegression()\n",
    "\n",
    "# Créer un VotingClassifier en mixant le RandomForestClassifier et la LogisticRegression\n",
    "voting_clf = VotingClassifier(estimators=[('lr', voting_logisticRegr), ('rf', voting_forest)], voting='soft')\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_voting, tpr_voting, thresholds_voting, roc_auc_voting = display_scores(voting_clf, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle (combinaison d'une RandomForest et d'une régression linéaire) a de bons résultats. La précision est très haute et le recall est moyen pour un F1-score assez élevé. D'après la matrice de confusion, le nombre de faux positif est très faible mais le nombre de faux positif reste assez élevé mais toujours assez élevé. Le meilleur compromis entre recall et précision est environ à 0.9. Ce modèle semble également être bon dans notre situation mais la RandomForest reste le modèle le plus performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des différents modèles\n",
    "Dans cette section, nous avons sélectionné un modèle en se basant sur la performance globale des candidats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Courbe ROC\n",
    "La courbe ROC a été un outil essentiel dans notre processus de sélection du modèle. En analysant la capacité de chaque modèle à discriminer entre les classes, nous avons pu évaluer leur efficacité en termes de taux de faux positifs et de vrais positifs. Cette visualisation a joué un rôle clé dans la prise de décision finale en identifiant le modèle offrant le meilleur équilibre entre sensibilité et spécificité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.figure(figsize=(7, 5))\n",
    "plot_roc_curve(fpr_lgr, tpr_lgr, \"Régression logistique\")\n",
    "plot_roc_curve(fpr_voting, tpr_voting, \"Voting Classifier\")\n",
    "plot_roc_curve(fpr_clf, tpr_clf, \"Random Forest\")\n",
    "plot_roc_curve(fpr_sdg, tpr_sdg, \"SDG Classifier\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print('ROC AUC Régression logistique', roc_auc_lgr)\n",
    "print('ROC AUC Voting Classifier', roc_auc_voting)\n",
    "print('ROC AUC Random Forest', roc_auc_clf)\n",
    "print('ROC AUC SDGClassifier', roc_auc_sdg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après nos analyses, le modèle Random Forest surpasse les autres en termes de performance avec un score ROC AUC de 0.91, démontrant ainsi une meilleure capacité de discrimination. En revanche, les modèles de Régression Logistique et SDGClassifier présentent des scores plus faibles, respectivement 0.58 et 0.60, indiquant une précision de classification moins élevée. Le Voting Classifier se positionne de manière intermédiaire avec un score de 0.80.\n",
    "\n",
    "Ainsi, au vu des différents résultats entre les différents modèles, nous pouvons les classer comme suit en termes de performance :\n",
    "\n",
    "| Position | Modèle               |\n",
    "|----------|----------------------|\n",
    "| 1er      | RandomForest        |\n",
    "| 2ème     | VotingClassifier     |\n",
    "| 3ème     | Régression logistique|\n",
    "| 4ème     | SDG Classifier       |\n",
    "\n",
    "Nous allons donc, par la suite de ce notebook, améliorer notre meilleur modèle, le **RandomForest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de la méthode GridSearchCV afin de trouver les meilleurs hyperparamètres pour notre modèle de RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Définir les paramètres\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], \n",
    "    'max_depth': [None, 10, 20, 30, 40],  \n",
    "    'min_samples_split': [2, 5, 10, 15],  \n",
    "    'min_samples_leaf': [1, 2, 4],  \n",
    "    'bootstrap': [True, False],  \n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  \n",
    "    'criterion': ['gini', 'entropy'] \n",
    "}\n",
    "\n",
    "# Créer un GridSearchCV\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Effectuer la recherche par grille sur les données d'entraînement\n",
    "grid_search.fit(X_train, y_train_yes)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la performance associée\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleur score de validation croisée : \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer l'importance de chaque paramètre pour notre modèle\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# Créer une liste de tuples avec l'importance de chaque paramètre et son nom\n",
    "importance_tuples = sorted(zip(feature_importances, X.columns), reverse=True)\n",
    "\n",
    "indices = np.argsort(feature_importances)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification du modèle sur le jeu de Test\n",
    "\n",
    "Nous vérifions les performances de notre meilleur modèle sur notre jeu de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les données de test\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "def show_score(pred):\n",
    "    # Calcule des différents indicateurs de performance\n",
    "    conf_matrix = confusion_matrix(y_test_yes, pred)\n",
    "    precision = precision_score(y_test_yes, pred)\n",
    "    recall = recall_score(y_test_yes, pred)\n",
    "    f1 = f1_score(y_test_yes, pred)\n",
    "    accuracy = accuracy_score(y_test_yes, pred)\n",
    "    \n",
    "\n",
    "    # Affichage des indicateurs de performance\n",
    "    print('Précision : ', precision)\n",
    "    print('Recall : ', recall)\n",
    "    print('F1-score : ', f1)\n",
    "    print('Accuracy : ', accuracy)\n",
    "\n",
    "\n",
    "    plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "show_score(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(grid_search,\"HumanForYou_RandomForestCollection_Kernel.pkl\")\n",
    "my_loaded_model = joblib.load(\"HumanForYou_RandomForestCollection_Kernel.pkl\")\n",
    "\n",
    "\n",
    "prediction = my_loaded_model.predict(X_test)\n",
    "show_score(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Dans cette section, nous avons tout d’abord divisé les données en jeu d’entrainement et de test. Par la suite, nous avons exploré différents modèles de classifications tels que la classification logistique ou le RandomForestClassifier. Nous les avons ensuite entrainés et évalués afin de trouver quel modèle été le plus adapté pour identifier les facteurs ayant le plus d'influence sur le taux de rotation. Après analyse des résultats des différents modèles, nous pouvons en conclure que le modèle le plus efficace est le RandomforestClassifier. Avec ce dernier on trouve une précision très importante dans notre jeu de données d’entrainement et parfaite dans celui de test. \n",
    "Par la suite, nous avons procédé à l'identification des facteurs ayant le plus d'influence sur le taux de rotation en réalisant une corrélation.\n",
    "Grâce à cela nous pouvons en déduire que le paramètre « MedianWorkingTime » est le facteur le plus influent sur le taux de départ ou non des employés de l’entreprise. Il est suivi par « TotalWorkingYears » et « MonthlyIncome ».\n",
    "Pistes d’améliorations : \n",
    "Ces facteurs indiquent donc que pour faire baisser le taux de rotation des employés, il faut faire baisser la médiane du temps de travail par exemple. Cela nous indique aussi que plus l’employé travail depuis longtemps moins il a de chances de vouloir quitter l’entreprise.\n",
    "Pour faire baisser le taux de rotation il faudrait donc diminuer le temps de travail, embaucher des employés qui ont déjà une plusieurs années de travails à leur actif ou encore augmenter les salaires."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
