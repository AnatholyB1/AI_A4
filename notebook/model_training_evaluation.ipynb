{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA for HumanForYou - Entraînement et Évaluation du Modèle\n",
    "\n",
    "|Auteur|\n",
    "|---|\n",
    "|G. DUBOYS DE LAVIGERIE|\n",
    "|T. VILLETTE|\n",
    "|O. BOUSSARD|\n",
    "|A. BRICON|\n",
    "\n",
    "## Objectifs du Notebook\n",
    "\n",
    "Ce notebook a pour objectif de développer plusieurs modèles d'IA en utilisant les données prétraitées et de les évaluer sur un ensemble de test. Les étapes comprennent :\n",
    "\n",
    "1. **Division des données :** Séparation des données en ensembles d'entraînement et de test.\n",
    "2. **Entraînement des modèles :** Développement de plusieurs modèles d'IA avec les données d'entraînement.\n",
    "3. **Évaluation des modèles :** Mesure des performances de chaque modèle sur l'ensemble de test.\n",
    "4. **Sélection du meilleur modèle :** Analyse des résultats pour choisir le modèle offrant les performances les plus prometteuses.\n",
    "\n",
    "## Attendus\n",
    "\n",
    "À la fin de ce notebook, le modèle sera prêt à être déployé, accompagné d'une évaluation détaillée de ses performances.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir exécuté le notebook `data_preprocessing.ipynb` pour garantir que les données sont prêtes pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilité entre Python 2 et Python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Imports nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assurer la stabilité du notebook entre différentes exécutions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Affichage de figures directement dans le notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ajustements des paramètres de l'affichage des figures\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Ignorer les avertissements inutiles (voir SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données traitées réalisée dans le notebook `data_preprocessing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) # Retirer la limite de colonnes à afficher\n",
    "\n",
    "def load_preprocessed_data(file_path='../datasets/data_preprocessing.csv'):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Charger les données prétraitées\n",
    "result = load_preprocessed_data()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division des données (jeu d'entraînement, jeu de test)\n",
    "Cette étape consiste à séparer les données en un jeu d'entraînement (80%) et un jeu de test (20%) afin d'évaluer la capacité du modèle à généraliser sur de nouvelles données. \n",
    "\n",
    "- Le **jeu d'entraînement** est utilisé pour former le modèle, lui fournissant une quantité suffisante de données pour apprendre les motifs et les relations présents dans nos données.\n",
    "\n",
    "- Le **jeu de test** permet de mesurer la performance du modèle sur des données non vues pendant l'entraînement, offrant ainsi une évaluation impartiale de sa capacité de généralisation.\n",
    "\n",
    "Le choix de la division 80/20 n'est pas arbitraire. Il s'agit d'une règle empirique bien établie dans le domaine de l'apprentissage machine, offrant un équilibre optimal entre la taille de l'ensemble d'entraînement nécessaire pour un apprentissage efficace et la capacité à évaluer le modèle de manière significative sur le jeu de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supposons que nous voulons prédire 'cat_nom__Attrition_Yes'\n",
    "y = result['Attrition']\n",
    "X = result.drop(['Attrition'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Diagramme pour le jeu d'entraînement\n",
    "plt.figure(figsize=(8, 5))\n",
    "y_train.value_counts().plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Répartition de Attrition dans le jeu d\\'entraînement')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Nombre d\\'employés')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Diagramme pour le jeu de test\n",
    "plt.figure(figsize=(8, 5))\n",
    "y_test.value_counts().plot(kind='bar', color=['blue', 'red'])\n",
    "plt.title('Répartition de Attrition dans le jeu de test')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Nombre d\\'employés')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un graphique\n",
    "fig, ax = plt.subplots(figsize=(2, 5))\n",
    "\n",
    "rect_train = plt.Rectangle((0, 0), 1, 0.8, color='red', alpha=0.5, label='Entraînement')\n",
    "rect_test = plt.Rectangle((0, 0.8), 1, 0.2, color='blue', alpha=0.5, label='Test')\n",
    "\n",
    "ax.add_patch(rect_train)\n",
    "ax.add_patch(rect_test)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Répartition des Données\")\n",
    "\n",
    "ax.text(0.5, 0.4, f'{len(X_train)}', ha='center', va='center', color='black', fontsize=8)\n",
    "ax.text(0.5, 0.9, f'{len(X_test)}', ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "ax.legend(loc='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir et Entraîner un modèle\n",
    "\n",
    "Avant de plonger dans les détails, il est essentiel de comprendre comment choisir, entraîner et évaluer un modèle. Une étape cruciale consiste à analyser les performances du modèle sur un ensemble de test et à utiliser des techniques comme la validation croisée pour garantir une évaluation robuste du modèle.\n",
    "\n",
    "**Fonction de validation croisée**\n",
    "\n",
    "La fonction `train_evaluate_model` ci-dessous illustre un processus générique d'entraînement et d'évaluation de modèles de régression. Cela garantit une approche standardisée dans notre notebook, simplifiant ainsi le processus et assurant une cohérence d'évaluation, indépendamment de l'algorithme sous-jacent.\n",
    "\n",
    "En utilisant la fonction `train_evaluate_model`, nous appliquons la validation croisée avec deux métriques :\n",
    "- **RMSE (Root Mean Square Error)** : Mesure la moyenne des erreurs au carré entre prédictions et valeurs réelles, avec prise de la racine carrée de cette moyenne.\n",
    "- **MAE (Mean Absolute Error)** : Autre métrique mesurant la moyenne des valeurs absolues des erreurs entre prédictions et valeurs réelles.\n",
    "\n",
    "Cette approche renforce l'évaluation des modèles en utilisant `scikit-learn` pour calculer le RMSE, la MAE, et la validation croisée, fournissant une évaluation complète de la performance du modèle.\n",
    "\n",
    "Il convient de noter que nous utilisons cette fonction pour chacun de nos modèles, assurant ainsi une comparaison équitable des performances entre les différentes approches algorithmiques que nous explorons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_model(trainer):\n",
    "    # Entraînement du modèle\n",
    "    trainer.fit(X_train, y_train)\n",
    "    \n",
    "    some_data = X_train.iloc[:5]\n",
    "    some_labels = y_train.iloc[:5]\n",
    "\n",
    "    # Et on effectue la prédiction :\n",
    "    print(\"Predictions:\", trainer.predict(some_data))\n",
    "    print(\"Labels:\", list(some_labels)) # vraies valeurs\n",
    "\n",
    "    # Prédictions sur les données de test\n",
    "    trainer_predictions = trainer.predict(X_test)\n",
    "    # Évaluation des prédictions sur les données de test\n",
    "    trainer_mse = mean_squared_error(y_test, trainer_predictions)\n",
    "    trainer_rmse = np.sqrt(trainer_mse)\n",
    "\n",
    "    # Affichage des métriques d'évaluation\n",
    "    print(\"MSE:\", trainer_mse)\n",
    "    print(\"RMSE:\", trainer_rmse)\n",
    "\n",
    "    # Validation croisée pour évaluer la performance du modèle\n",
    "    scores = cross_val_score(trainer, X_test, y_test,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    trainer_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "    # Affichage des scores de validation croisée\n",
    "    print(\"Cross-Validation Scores:\", trainer_rmse_scores)\n",
    "\n",
    "    # Affichage de la performance moyenne et de la variabilité du modèle\n",
    "    print(\"Mean RMSE:\", trainer_rmse_scores.mean())\n",
    "    print(\"RMSE Standard Deviation:\", trainer_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modèle de régression linéaire</u>\n",
    "\n",
    "Entraînons un modèle de [_régression linéaire_](https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire) avec nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "train_model(lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modèle de régression par arbre de décision (DecisionTreeRegressor) </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "train_model(tree_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modèle RandomForestRegressor</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(max_depth=15, max_features='log2', n_estimators=26,\n",
    "                      random_state=42)\n",
    "train_model(forest_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Affiche le graphique de la précision et recall en fonction du seuil\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Affiche le graphique de la précision en fonction du recall\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recalls, precisions, \"k-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Entraîner et évaluer le modèle puis afficher les scores\n",
    "def display_scores(model, x, y, method):\n",
    "    model.fit(x, y) # Entraîner le modèle\n",
    "\n",
    "    y_train_pred = cross_val_predict(model, x, y, cv=3) # Validation croisée\n",
    "    y_scores = cross_val_predict(model, x, y, cv=5, method=method) # Calcul des scores\n",
    "\n",
    "    # Calcul des différents indicateurs de performance\n",
    "    conf_matrix = confusion_matrix(y, y_train_pred)\n",
    "    precision = precision_score(y, y_train_pred)\n",
    "    recall = recall_score(y, y_train_pred)\n",
    "    f1 = f1_score(y, y_train_pred)\n",
    "    accuracy = accuracy_score(y, y_train_pred)\n",
    "\n",
    "\n",
    "    # Si method est \"decision_function\", y_scores est un tableau 1D\n",
    "    if method == \"decision_function\":\n",
    "        # calculer précisions, recalls et seuils pour la courbe ROC\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "        a, b, c = roc_curve(y, y_scores)\n",
    "    # Si method est \"predict_proba\", y_scores est un tableau 2D\n",
    "    else:\n",
    "        # calculer précisions, recalls et seuils pour la courbe ROC\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y, y_scores[:, -1])\n",
    "        a, b, c = roc_curve(y, y_scores[:, -1])\n",
    "\n",
    "    # Affichage des indicateurs de performance\n",
    "    print('Matrice de confusion : ', conf_matrix)\n",
    "    print('Précision : ', precision)\n",
    "    print('Recall : ', recall)\n",
    "    print('F1-score : ', f1)\n",
    "    print('Accuracy : ', accuracy)\n",
    "\n",
    "    # Affichage des graphiques\n",
    "    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "    plot_precision_vs_recall(precisions, recalls)\n",
    "    \n",
    "    # retourner précisions, recalls et seuils pour la courbe ROC\n",
    "    return a, b, c\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mélange de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train.iloc[shuffle_index], y_train.iloc[shuffle_index]\n",
    "\n",
    "# Garder les données avec attrition à True (> 0)\n",
    "y_train_yes = (y_train > 0)\n",
    "y_test_yes = (y_test > 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SDG Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Créer un SGDClassifier\n",
    "sgd_clf = SGDClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_sdg, tpr_sdg, thresholds_sdg = display_scores(sgd_clf, X_train, y_train_yes, \"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Créer un RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_clf, tpr_clf, thresholds_clf = display_scores(forest_clf, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Créer une LogistiqueRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_lgr, tpr_lgr, thresholds_lgr = display_scores(logisticRegr, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  VotingClassifier\n",
    "\n",
    "# Créer un RandomForestClassifier et un LogisticRegression\n",
    "voting_forest = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "voting_logisticRegr = LogisticRegression()\n",
    "\n",
    "# Créer un VotingClassifier en mixant le RandomForestClassifier et la LogisticRegression\n",
    "voting_clf = VotingClassifier(estimators=[('lr', voting_logisticRegr), ('rf', voting_forest)], voting='soft')\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "fpr_voting, tpr_voting, thresholds_voting = display_scores(voting_clf, X_train, y_train_yes, \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparer les modèles \n",
    "Courbe ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lgr, tpr_lgr, \"b:\", linewidth=2, label=\"Régression logistique\")\n",
    "plot_roc_curve(fpr_voting, tpr_voting, \"Voting\")\n",
    "plot_roc_curve(fpr_clf, tpr_clf, \"Random Forest\")\n",
    "plot_roc_curve(fpr_sdg, tpr_sdg, \"SDG Classifier\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch\n",
    "Utilisation de la méthode GridSearchCV afin de trouver les meilleurs hyperparamètres pour un modèle de régression RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Définir les paramètres\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Créer un GridSearchCV\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Effectuer la recherche par grille sur les données d'entraînement\n",
    "grid_search.fit(X_train, y_train_yes)\n",
    "\n",
    "# Afficher les meilleurs paramètres et la performance associée\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "print(\"Meilleur score de validation croisée : \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer l'importance de chaque paramètre pour notre modèle\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "# Afficher l'importance de chaque paramètre\n",
    "sorted(zip(feature_importances, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(feature_importances)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_yes, y_test_pred)\n",
    "precision = precision_score(y_test_yes, y_test_pred)\n",
    "recall = recall_score(y_test_yes, y_test_pred)\n",
    "f1 = f1_score(y_test_yes, y_test_pred)\n",
    "accuracy = accuracy_score(y_test_yes, y_test_pred)\n",
    "\n",
    "# Affichage des indicateurs de performance\n",
    "print('Matrice de confusion : ', conf_matrix)\n",
    "print('Précision : ', precision)\n",
    "print('Recall : ', recall)\n",
    "print('F1-score : ', f1)\n",
    "print('Accuracy : ', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
