{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA for HumanForYou\n",
    "\n",
    "|Auteur|\n",
    "|---|\n",
    "|G. DUBOYS DE LAVIGERIE|\n",
    "|T. VILETTE|\n",
    "|O. BOUSSARD|\n",
    "|A. BRICON|\n",
    "\n",
    "## Objectifs du Livrable\n",
    "\n",
    "Le présent document constitue le premier livrable du projet IA pour HumanForYou, visant à analyser et traiter les différentes données fournies par l'entreprise dans le but de comprendre les facteurs influençant le taux de rotation des employés. Les objectifs principaux de ce livrable sont les suivants :\n",
    "\n",
    "1. **Compréhension des données :** Explorer et comprendre les différentes sources de données fournies par HumanForYou, y compris les données des ressources humaines, les évaluations des managers, les enquêtes sur la qualité de vie au travail et les horaires de travail.\n",
    "\n",
    "2. **Prétraitement des données :** Nettoyer les données en éliminant les valeurs manquantes, en identifiant et en traitant les éventuelles erreurs ou incohérences, et en préparant les données pour l'analyse et la modélisation.\n",
    "\n",
    "3. **Exploration des données :** Effectuer une analyse exploratoire des données pour identifier les tendances, les relations et les motifs significatifs pouvant influencer le taux de rotation des employés.\n",
    "\n",
    "4. **Visualisation des données :** Utiliser des techniques de visualisation de données pour représenter graphiquement les principales caractéristiques des données et faciliter la compréhension des résultats.\n",
    "\n",
    "5. **Préparation des données pour la modélisation :** Préparer les données en sélectionnant les variables pertinentes, en transformant les variables catégorielles en variables numériques, et en divisant les données en ensembles d'entraînement et de test pour la modélisation.\n",
    "\n",
    "## Attendus\n",
    "\n",
    "À la fin de ce livrable, nous nous attendons à ce que les données fournies par HumanForYou soient prêtes à être analysées et utilisées pour la modélisation. Nous fournirons une analyse détaillée de la qualité des données, des tendances et des relations identifiées, ainsi qu'une documentation complète sur les étapes de prétraitement des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Préparation de l'environnement\n",
    "\n",
    "Pour faciliter l'importation des bibliothèques nécessaires au bon fonctionnement du code, veuillez exécuter le fichier `setup.ipynb`. Ce fichier s'occupe de configurer l'environnement en important toutes les librairies essentielles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibilité python 2 et python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# stabilité du notebook d'une exécution à l'autre\n",
    "np.random.seed(42)\n",
    "\n",
    "# jolies figures directement dans le notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# où sauver les figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"workflowDS\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID) # le dossier doit exister\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# ignorer les warnings inutiles (voir SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import des données\n",
    "\n",
    "Dans cette section, nous automatisons le processus d'importation des données en implémentant une fonction qui effectue les étapes suivantes :\n",
    "1. Téléchargement de l'archive contenant les fichiers.\n",
    "2. Extraction des fichiers de l'archive.\n",
    "\n",
    "Le code ci-dessous réalise le chargement des fichiers suivants :\n",
    "- `employee_survey_data.csv`\n",
    "- `general_data.csv`\n",
    "- `in_time.csv`\n",
    "- `out_time.csv`\n",
    "- `manager_survey_data.csv`\n",
    "\n",
    "De même, on va créer une fonction utilisant [`pandas`](https://pandas.pydata.org/) qui charge les données en mémoire dans un [`Pandas DataFrame`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/\"\n",
    "REPO_PATH = \"AnatholyB1/AI_A4/main/\"\n",
    "DATA_PATH = os.path.join(\"../datasets\", \"all\")\n",
    "DATA_URL = DOWNLOAD_ROOT + REPO_PATH + \"data.zip\"\n",
    "\n",
    "\n",
    "def fetch_data(data_url=DATA_URL, data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    zip_path = os.path.join(data_path, \"data.zip\")\n",
    "    urllib.request.urlretrieve(data_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "fetch_data()\n",
    "\n",
    "def load_employee_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\employe\", \"employee_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_general_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\general\", \"general_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_in_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"in_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_out_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"out_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_manager_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\manager\", \"manager_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "employee = load_employee_data()\n",
    "general = load_general_data()\n",
    "in_time = load_in_time_data()\n",
    "out_time = load_out_time_data()\n",
    "manager = load_manager_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyse des différentes données\n",
    "\n",
    "#### 3.1. employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Créez un pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "\n",
    "# Appliquez le pipeline à l'ensemble du DataFrame\n",
    "employee_transformed = numeric_transformer.fit_transform(employee)\n",
    "\n",
    "# Transformez le résultat du pipeline en DataFrame\n",
    "employee_df = pd.DataFrame(employee_transformed, columns = employee.columns)\n",
    "employee_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 General\n",
    "\n",
    "| **Variable**           | **Type de données** | **Nombre de valeurs Manquantes** | **Nature de la variable** | **Nombre de catégories uniques** |\n",
    "|------------------------|---------------------|----------------------------------|---------------------------|----------------------------------|\n",
    "| **Age**                | Quantitative        | 0                                | Ordinal                   | 35                               |\n",
    "| **Attrition**          | Qualitative         | 0                                | Nominal (Booléen)         | 2                                |\n",
    "| **BusinessTravel**     | Qualitative         | 0                                | Ordinal                   | 3                                |\n",
    "| **Department**         | Qualitative         | 0                                | Nominal                   | 3                                |\n",
    "| **DistanceFromHome**   | Quantitative        | 0                                | Ordinal                   | 16                               |\n",
    "| **Education**          | Quantitative        | 0                                | Ordinal                   | 5                                |\n",
    "| **EducationField**     | Qualitative         | 0                                | Nominal                   | 6                                |\n",
    "| **EmployeeCount**      | Quantitative        | 0                                | Ordinal                   | 1                                |\n",
    "| **EmployeeID**         | Quantitative        | 0                                | Ordinal                   | 4410                             |\n",
    "| **Gender**             | Qualitative         | 0                                | Nominal (Booléen)         | 2                                |\n",
    "| **JobLevel**           | Quantitative        | 0                                | Ordinal                   | 5                                |\n",
    "| **JobRole**            | Qualitative         | 0                                | Nominal                   | 9                                |\n",
    "| **MaritalStatus**      | Qualitative         | 0                                | Nominal                   | 3                                |\n",
    "| **MonthlyIncome**      | Quantitative        | 0                                | Ordinal                   | 1349                             |\n",
    "| **NumCompaniesWorked** | Quantitative        | 19                               | Ordinal                   | 10                               |\n",
    "| **Over18**             | Qualitative         | 0                                | Nominal                   | 1                                |\n",
    "| **PercentSalaryHike**  | Quantitative        | 0                                | Ordinal                   | 14                               |\n",
    "| **StandardHours**      | Quantitative        | 0                                | Ordinal                   | 1                                |\n",
    "| **StockOptionLevel**   | Quantitative        | 0                                | Ordinal                   | 4                                |\n",
    "| **TotalWorkingYears**  | Quantitative        | 9                                | Ordinal                   | 29                               |\n",
    "| **TrainingTimesLastYear** | Quantitative      | 0                                | Ordinal                   | 7                                |\n",
    "| **YearsAtCompany**     | Quantitative        | 0                                | Ordinal                   | 29                               |\n",
    "| **YearsSinceLastPromotion** | Quantitative    | 0                                | Ordinal                   | 15                               |\n",
    "| **YearsWithCurrManager** | Quantitative      | 0                                | Ordinal                   | 16                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer useless colonnes\n",
    "general_dr = general.drop(['Over18', 'EmployeeCount', 'StandardHours'], axis=1)\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "general_num = general_dr.select_dtypes(include=[np.number]) \n",
    "num_attribs = list(general_num)\n",
    "\n",
    "general_cat_ordinal = ['BusinessTravel']\n",
    "general_cat_nominal = ['Attrition','Department','EducationField','Gender','JobRole','MaritalStatus']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat_nom\", OneHotEncoder(), general_cat_nominal),\n",
    "    (\"cat_ord\", OrdinalEncoder(), general_cat_ordinal),\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline au DataFrame initial\n",
    "general_tr = full_pipeline.fit_transform(general_dr)\n",
    "\n",
    "# Créer un DataFrame à partir du résultat\n",
    "general_prepared = pd.DataFrame(general_tr, columns=full_pipeline.get_feature_names_out())\n",
    "general_prepared.drop(['cat_nom__Attrition_No'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 In_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_all_to_datetime(df):\n",
    "    year_dict = {}\n",
    "    month_dict = {}\n",
    "    day_dict = {}\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        try:\n",
    "            df[column_name] = pd.to_datetime(df[column_name])\n",
    "            year_dict[column_name + '-year'] = df[column_name].dt.year\n",
    "            month_dict[column_name + '-month'] = df[column_name].dt.month\n",
    "            day_dict[column_name + '-day'] = df[column_name].dt.day\n",
    "        except ValueError:\n",
    "            # Skip columns that cannot be converted to datetime\n",
    "            pass\n",
    "\n",
    "    year_df = pd.DataFrame(year_dict)\n",
    "    month_df = pd.DataFrame(month_dict)\n",
    "    day_df = pd.DataFrame(day_dict)\n",
    "\n",
    "    df = pd.concat([df, year_df, month_df, day_df], axis=1)\n",
    "    return df\n",
    "\n",
    "in_time = convert_all_to_datetime(in_time)\n",
    "out_time = convert_all_to_datetime(out_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suprimmer les colones qui ne sont pas des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def convert_all_to_datetime(X,  Y):    \n",
    "    cols_to_drop = X.filter(regex='-day$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-month$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-year$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = X.columns[0]\n",
    "    X = X.drop([first_column], axis=1)\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    X = X.dropna(axis=1, how='all')\n",
    "\n",
    "    cols_to_drop = Y.filter(regex='-day$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-month$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-year$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = Y.columns[0]\n",
    "    Y = Y.drop([first_column], axis=1)\n",
    "\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    Y = Y.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "    # Calculer la médiane du temps horaire pour chaque personne\n",
    "    median_time = (Y - X).median()\n",
    "\n",
    "    for column in X.columns:\n",
    "        # Trouver où les valeurs sont manquantes\n",
    "        in_time_nan = X[column].isna()\n",
    "        out_time_nan = Y[column].isna()\n",
    "\n",
    "        # Si 'in_time' est manquant, soustraire la médiane du temps horaire à 'out_time'\n",
    "        X.loc[in_time_nan, column] = X.loc[in_time_nan, column] - median_time[column]\n",
    "\n",
    "        # Si 'out_time' est manquant, ajouter la médiane du temps horaire à 'in_time'\n",
    "        Y.loc[out_time_nan, column] = Y.loc[out_time_nan, column] + median_time[column]\n",
    "    \n",
    "    return  Y - X\n",
    "\n",
    "\n",
    "\n",
    "class PreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].apply(lambda x: x.total_seconds() / 3600 if pd.notnull(x) else np.nan)    \n",
    "        stats = pd.DataFrame()\n",
    "        stats['mean'] = pd.Series(X.mean(axis=1))\n",
    "        stats['median'] = pd.Series(X.median(axis=1))\n",
    "        stats['min'] = pd.Series(X.min(axis=1))\n",
    "        stats['max'] = pd.Series(X.max(axis=1))\n",
    "        return stats, stats.columns\n",
    "        # Convertir les timedelta en une quantité numérique représentant le nombre d'heures\n",
    "\n",
    "\n",
    "# Définir le pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('pre_transformer', PreTransformer()),\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline\n",
    "hourly_time_prepared, stats_col = num_pipeline.fit_transform(convert_all_to_datetime(in_time, out_time))\n",
    "\n",
    "hourly_time_prepared = pd.DataFrame(hourly_time_prepared, columns= stats_col )\n",
    "\n",
    "#Ajouter la colonne 'EmployeeID' pour pouvoir joindre les DataFrames\n",
    "hourly_time_prepared['EmployeeID'] = employee_df['EmployeeID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser la fonction merge pour faire la première jointure avec general\n",
    "result = pd.merge(employee_df, manager, on='EmployeeID')\n",
    "\n",
    "# Utiliser la fonction merge pour faire la deuxième jointure avec general_prepared\n",
    "result = pd.merge(result, general_prepared, left_on='EmployeeID', right_on='num__EmployeeID')\n",
    "\n",
    "# Utiliser la fonction merge pour faire la troisieme jointure avec hourly_time_prepared\n",
    "result = pd.merge(result, hourly_time_prepared, on='EmployeeID')\n",
    "\n",
    "result = result.drop(['EmployeeID'], axis=1)\n",
    "result = result.drop(['num__EmployeeID'], axis=1)\n",
    "# Créez un pipeline\n",
    "result_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "result_array = result_transformer.fit_transform(result)\n",
    "result = pd.DataFrame(result_array, columns = result.columns)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supposons que vous voulez prédire 'cat_nom__Attrition_Yes'\n",
    "y = result['cat_nom__Attrition_Yes']\n",
    "X = result.drop(['cat_nom__Attrition_Yes', 'cat_nom__Attrition_No'], axis=1)\n",
    "\n",
    "# Diviser les données en un ensemble d'entraînement et un ensemble de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.DataFrame(pd.concat([X_train, y_train], axis = 1), columns=result.columns)\n",
    "\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr_matrix = train_df.corr()\n",
    "# Filtrer les corrélations\n",
    "filtered_corr_matrix = corr_matrix.where((corr_matrix >= 0.4) | (corr_matrix <= -0.4))\n",
    "\n",
    "# Trier la colonne 'cat_nom__Attrition_Yes' par valeur absolue\n",
    "sorted_corr = corr_matrix['cat_nom__Attrition_Yes'].apply(abs).sort_values(ascending=False)\n",
    "\n",
    "# Afficher les éléments les plus corrélés avec 'cat_nom__Attrition_Yes'\n",
    "sorted_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Create a DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "def train_model(trainer):\n",
    "    trainer.fit(X_train, y_train)\n",
    "\n",
    "    trainer_predictions = trainer.predict(X_test)\n",
    "    trainer_mse = mean_squared_error(y_test, trainer_predictions)\n",
    "    trainer_rmse = np.sqrt(trainer_mse)\n",
    "\n",
    "    print(\"mse:\", trainer_mse)\n",
    "    print(\"rmse:\", trainer_rmse)\n",
    "\n",
    "    scores = cross_val_score(trainer, X_test, y_test,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    trainer_rmse_scores = np.sqrt(-scores)\n",
    "    \n",
    "    print(\"Scores:\", trainer_rmse_scores)\n",
    "    print(\"Mean:\", trainer_rmse_scores.mean())\n",
    "    print(\"Standard deviation:\", trainer_rmse_scores.std())\n",
    "\n",
    "\n",
    "train_model(tree_reg)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression(fit_intercept=False)\n",
    "train_model(lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(max_depth=15, max_features='log2', n_estimators=26,\n",
    "                      random_state=42)\n",
    "train_model(forest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_reg = SVR(C=9.318742350231167, gamma=0.09849250205191949)\n",
    "train_model(svr_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # essaye 12 (3×4) combinaisons des hyperparametres\n",
    "    {'n_estimators': [3,7, 10,15,20,25 ,30], 'max_features': [2, 4, 6, 8]},\n",
    "    # puis essaye 6 (2×3) combinaisons avec bootstrap à False (True étant la valeur par défaut)\n",
    "    {'bootstrap': [False], 'n_estimators': [3,7, 10,15,20,25 ,30], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    " \n",
    "# 5 sous-jeux de cross-val, ça fait en tout (12+6)*5=90 tours d'entraînement \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(X_train,y_train )\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = grid_search.best_estimator_.feature_importances_\n",
    "attributes = result.columns\n",
    "sorted(zip(feature_importance, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = [\n",
    "    # essaye 12 (3×4) combinaisons des hyperparametres\n",
    "    {'kernel': ['linear'], 'C': [10., 30., 100.]},\n",
    "    # puis essaye 6 (2×3) combinaisons avec le noyau RBF\n",
    "    {'kernel': ['rbf'], 'C': [10., 30., 100.],'gamma': [0.3, 1.0, 3.0]},\n",
    "]\n",
    "\n",
    "svr_reg = SVR()\n",
    "\n",
    "# 5 sous-jeux de cross-val, ça fait en tout (8+6*6)*5=350 tours d'entraînement \n",
    "grid_search = GridSearchCV(svr_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "param_grid = [\n",
    "    {'fit_intercept': [True, False]}\n",
    "]\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "grid_search = GridSearchCV(lin_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = [\n",
    "    {'max_depth': [None, 5, 10, 15, 20], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 5]}\n",
    "]\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(tree_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Paramètres pour SVR\n",
    "param_distributions_svr = {\n",
    "    'C': uniform(0.1, 10),\n",
    "    'gamma': uniform(0.01, 1),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(svr, param_distributions=param_distributions_svr, \n",
    "                                       n_iter=500, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search_svr.fit(X_train, y_train)\n",
    "\n",
    "print(random_search_svr.best_params_)\n",
    "print(random_search_svr.best_estimator_)\n",
    "\n",
    "# Paramètres pour RandomForestRegressor\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': randint(2, 20),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_distributions_rf, \n",
    "                                      n_iter=500, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(random_search_rf.best_params_)\n",
    "print(random_search_rf.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
