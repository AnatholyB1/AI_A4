{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA for HumanForYou - Prétraitement des Données\n",
    "\n",
    "|Auteur|\n",
    "|---|\n",
    "|G. DUBOYS DE LAVIGERIE|\n",
    "|T. VILLETTE|\n",
    "|O. BOUSSARD|\n",
    "|A. BRICON|\n",
    "\n",
    "## Objectifs du Livrable\n",
    "\n",
    "Ce notebook vise à prétraiter les différentes sources de données fournies par HumanForYou. Les principales étapes comprennent :\n",
    "\n",
    "1. **Nettoyage des données :** Élimination des valeurs manquantes, correction d'erreurs potentielles.\n",
    "2. **Transformation des données :** Préparation des données pour l'analyse et la modélisation.\n",
    "3. **Création de nouvelles caractéristiques :** Le cas échéant, créer de nouvelles caractéristiques basées sur les données existantes.\n",
    "\n",
    "## Attendus\n",
    "\n",
    "À la fin de ce notebook, les données seront prêtes à être explorées et utilisées pour l'analyse et le développement du modèle d'IA.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir exécuté le notebook `setup.ipynb` pour garantir que la configuration de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de l'environnement\n",
    "\n",
    "Ci-dessous quelques imports et précautions préalables à notre travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilité python 2 et python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Import de Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Stabilité du notebook d'une exécution à l'autre\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paramétrer le rendu des figures affichées dans le notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Ignorer les warnings inutiles\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des données\n",
    "\n",
    "Dans cette section, nous automatisons le processus d'importation des données en implémentant une fonction qui effectue les étapes suivantes :\n",
    "1. Téléchargement de l'archive contenant les fichiers.\n",
    "2. Extraction des fichiers de l'archive.\n",
    "\n",
    "Le code ci-dessous réalise le chargement des fichiers suivants :\n",
    "- `employee_survey_data.csv`\n",
    "- `general_data.csv`\n",
    "- `in_time.csv`\n",
    "- `out_time.csv`\n",
    "- `manager_survey_data.csv`\n",
    "\n",
    "De même, on va créer une fonction utilisant [`pandas`](https://pandas.pydata.org/) qui charge les données en mémoire dans un [`Pandas DataFrame`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import zipfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Retirer la limite de colonnes à afficher\n",
    "\n",
    "# Path\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/\"\n",
    "REPO_PATH = \"AnatholyB1/AI_A4/main/\"\n",
    "DATA_PATH = os.path.join(\"../datasets\", \"all\")\n",
    "DATA_URL = DOWNLOAD_ROOT + REPO_PATH + \"data.zip\"\n",
    "\n",
    "# Récupérer et extraire les fichiers de données\n",
    "def fetch_data(data_url=DATA_URL, data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    zip_path = os.path.join(data_path, \"data.zip\")\n",
    "    urllib.request.urlretrieve(data_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "fetch_data()\n",
    "\n",
    "# Charger les fichiers de données\n",
    "def load_employee_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\employe\", \"employee_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_general_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\general\", \"general_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_in_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"in_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_out_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"out_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_manager_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\manager\", \"manager_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "employee = load_employee_data()\n",
    "general = load_general_data()\n",
    "in_time = load_in_time_data()\n",
    "out_time = load_out_time_data()\n",
    "manager = load_manager_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration et prétraitement des fichiers de données\n",
    "\n",
    "Au sein de cette section, nous effectuons une analyse des fichiers de données dans le but de comprendre en détail leur composition. Des ajustements et transformations sont ensuite appliqués de manière rigoureuse, dans le but optimal de préparer ces données en vue de les adapter au modèle envisagé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fichier : `general_data.csv`\n",
    "##### Analyse des données\n",
    "\n",
    "Afin d'analyser les données, nous allons tout d'abord afficher les différentes informations en utilisant la méthode `.info()` de la bibliothèque Pandas. Cette méthode nous permettra de :\n",
    "\n",
    "- Visualiser les types de données présents dans chaque colonne.\n",
    "- Identifier les valeurs manquantes éventuelles.\n",
    "- Obtenir le nombre total d'entrées dans le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concernant les données du fichier `general_data.csv` :\n",
    "\n",
    "- Nous constatons que les colonnes \"*NumCompaniesWorked*\" et \"*TotalWorkingYears*\" comportent des valeurs manquantes.\n",
    "\n",
    "- De plus, il est important de noter que huit colonnes ne renferment pas de valeurs numériques.\n",
    "\n",
    "Ces observations soulignent la nécessité d'appliquer une gestion adéquate des valeurs manquantes et éventuellement de convertir les données non numériques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à cela, pour analyser les données, nous avons utilisé la méthode `.value_counts()` afin d'identifier, parmi les colonnes non quantitatives, le nombre de valeurs uniques pour chaque colonne. Cette approche nous permet de déduire si ces colonnes sont de nature ordinale ou nominale, apportant ainsi une compréhension approfondie de la structure des données et facilitant la prise de décisions quant aux étapes ultérieures de traitement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, pour une meilleure compréhension des données, nous avons effectué un affichage en histogramme pour chacun des fichiers afin d'observer les distributions des valeurs. D'après ces histogrammes, nos données ne semblent pas contenir de valeurs aberrantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Voici un tableau résumant toutes les informations :\n",
    "\n",
    "| **Variable**           | **Type de données** | **Nombre de valeurs Manquantes** | **Nature de la variable** | **Nombre de catégories uniques** |\n",
    "|------------------------|---------------------|----------------------------------|---------------------------|----------------------------------|\n",
    "| **Age**                | Quantitative        | 0                                | Ordinal                   | 35                               |\n",
    "| **Attrition**          | Qualitative         | 0                                | Nominal (Booléen)         | 2                                |\n",
    "| **BusinessTravel**     | Qualitative         | 0                                | Ordinal                   | 3                                |\n",
    "| **Department**         | Qualitative         | 0                                | Nominal                   | 3                                |\n",
    "| **DistanceFromHome**   | Quantitative        | 0                                | Ordinal                   | 16                               |\n",
    "| **Education**          | Quantitative        | 0                                | Ordinal                   | 5                                |\n",
    "| **EducationField**     | Qualitative         | 0                                | Nominal                   | 6                                |\n",
    "| **EmployeeCount**      | Quantitative        | 0                                | Ordinal                   | 1                                |\n",
    "| **EmployeeID**         | Quantitative        | 0                                | Ordinal                   | 4410                             |\n",
    "| **Gender**             | Qualitative         | 0                                | Nominal (Booléen)         | 2                                |\n",
    "| **JobLevel**           | Quantitative        | 0                                | Ordinal                   | 5                                |\n",
    "| **JobRole**            | Qualitative         | 0                                | Nominal                   | 9                                |\n",
    "| **MaritalStatus**      | Qualitative         | 0                                | Nominal                   | 3                                |\n",
    "| **MonthlyIncome**      | Quantitative        | 0                                | Ordinal                   | 1349                             |\n",
    "| **NumCompaniesWorked** | Quantitative        | 19                               | Ordinal                   | 10                               |\n",
    "| **Over18**             | Qualitative         | 0                                | Nominal                   | 1                                |\n",
    "| **PercentSalaryHike**  | Quantitative        | 0                                | Ordinal                   | 14                               |\n",
    "| **StandardHours**      | Quantitative        | 0                                | Ordinal                   | 1                                |\n",
    "| **StockOptionLevel**   | Quantitative        | 0                                | Ordinal                   | 4                                |\n",
    "| **TotalWorkingYears**  | Quantitative        | 9                                | Ordinal                   | 29                               |\n",
    "| **TrainingTimesLastYear** | Quantitative      | 0                                | Ordinal                   | 7                                |\n",
    "| **YearsAtCompany**     | Quantitative        | 0                                | Ordinal                   | 29                               |\n",
    "| **YearsSinceLastPromotion** | Quantitative    | 0                                | Ordinal                   | 15                               |\n",
    "| **YearsWithCurrManager** | Quantitative      | 0                                | Ordinal                   | 16                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Préparation des données\n",
    "\n",
    "L'analyse de ces données nous guide dans les modifications à apporter au jeu de données afin de le préparer au mieux pour le modèle de machine learning.\n",
    "\n",
    "Tout d'abord, nous supprimons les colonnes inutiles telles que \"Over18\", \"EmployeeCount\" et \"StandardHours\" car leur valeur est constante et n'influe donc pas sur le taux de rotation des employés.\n",
    "\n",
    "Ensuite, nous mettons en place un pipeline pour nettoyer et transformer les données. Ce pipeline se compose de deux étapes :\n",
    "- **Remplacer les valeurs nulles :**\n",
    "  Chaque valeur manquante est remplacée par la médiane de la colonne. Les valeurs ordinales sont numérisées à l'aide de la fonction SimpleImputer de scikit-learn.\n",
    "  \n",
    "- **Numériser les données qualitatives :**\n",
    "  Pour les valeurs qualitatives ordinales, nous utilisons l'OrdinalEncoder.\n",
    "  Pour les valeurs qualitatives nominales, nous faisons appel au OneHotEncoder. Cela génère une colonne booléenne pour chaque catégorie. Par exemple, la colonne \"Department\" sera décomposée en trois colonnes : \"Research & Development\", \"Sales\" et \"Human Resources\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes inutiles\n",
    "general_dr = general.drop(['Over18', 'EmployeeCount', 'StandardHours'], axis=1)\n",
    "\n",
    "# Préparer le pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "general_num = general_dr.select_dtypes(include=[np.number]) # DataFrame uniquement avec les valeurs numériques\n",
    "num_attribs = list(general_num) # Récupérer les noms des colonnes numériques\n",
    "\n",
    "general_cat_ordinal = ['BusinessTravel', 'Attrition', 'Gender'] # Colonnes qualitatives avec des valeurs ordinales\n",
    "general_cat_nominal = ['Department','EducationField','JobRole','MaritalStatus'] # Colonnes qualitatives avec des valeurs nominales\n",
    "\n",
    "# Pipeline numérique\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")), # Remplacer les valeurs nulles par ma médiane des valeurs de la colonne\n",
    "])\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat_nom\", OneHotEncoder(), general_cat_nominal), # Numériser les colonnes qualitatives avec des valeurs nominales\n",
    "    (\"cat_ord\", OrdinalEncoder(), general_cat_ordinal), # Numériser les colonnes qualitatives avec des valeurs ordinales\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline au DataFrame initial\n",
    "general_tr = full_pipeline.fit_transform(general_dr)\n",
    "\n",
    "# Créer un DataFrame à partir du résultat\n",
    "general_prepared = pd.DataFrame(general_tr, columns=full_pipeline.get_feature_names_out())\n",
    "general_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais constater qu'après transformation, toutes nos données sont numériques et qu'il n'y a plus de valeurs nulles. Nous allons suivre la même procédure pour les trois autres fichiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fichier : `employee_survey_data.csv`\n",
    "\n",
    "Nous analysons et préparons les données du fichier `employee_survey_data.csv` en utilisant la même méthode que dans la partie précédente. Nous avons observé la présence de quelques valeurs manquantes dans une colonne.\n",
    "\n",
    "La seule transformation appliquée dans ce fichier est le remplacement des valeurs nulles par la médiane. Voici le pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Créer un pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "\n",
    "# Appliquer le pipeline à l'ensemble du DataFrame\n",
    "employee_transformed = numeric_transformer.fit_transform(employee)\n",
    "\n",
    "# Transformer le résultat du pipeline en DataFrame\n",
    "employee_df = pd.DataFrame(employee_transformed, columns = employee.columns)\n",
    "employee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fichier : `in_time.csv` et `out_time.csv`\n",
    "\n",
    "Pour les fichiers `in_time.csv` et `out_time.csv`, toutes les valeurs sont converties en format DateTime, car elles sont plus simples à manipuler. Voici le code associé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir toutes les valeurs en DateTime\n",
    "def convert_all_to_datetime(df):\n",
    "    year_dict = {}\n",
    "    month_dict = {}\n",
    "    day_dict = {}\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        try:\n",
    "            df[column_name] = pd.to_datetime(df[column_name])\n",
    "            year_dict[column_name + '-year'] = df[column_name].dt.year\n",
    "            month_dict[column_name + '-month'] = df[column_name].dt.month\n",
    "            day_dict[column_name + '-day'] = df[column_name].dt.day\n",
    "        except ValueError:\n",
    "            # Passe les valeurs qui ne peuvent pas être converties en DateTime\n",
    "            pass\n",
    "\n",
    "    year_df = pd.DataFrame(year_dict)\n",
    "    month_df = pd.DataFrame(month_dict)\n",
    "    day_df = pd.DataFrame(day_dict)\n",
    "\n",
    "    df = pd.concat([df, year_df, month_df, day_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# Convertir en DateTime toutes les valeurs des fichiers Out_time et In_time\n",
    "in_time = convert_all_to_datetime(in_time)\n",
    "out_time = convert_all_to_datetime(out_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous utilisons un pipeline pour préparer les données au modèle de machine learning. Les actions du pipeline sont les suivantes :\n",
    "\n",
    "1. **Suppression des colonnes non liées aux dates :**\n",
    "   - Les colonnes qui ne représentent pas des dates sont supprimées, simplifiant ainsi le traitement ultérieur.\n",
    "\n",
    "2. **Remplacement des valeurs nulles :**\n",
    "   - Lorsqu'une valeur est manquante dans un fichier, elle peut être calculée grâce à la valeur correspondante dans l'autre fichier. Si les deux valeurs correspondantes sont nulles dans les deux fichiers, la ligne est supprimée.\n",
    "\n",
    "3. **Transformation des données :**\n",
    "   - Toutes les données sont transformées pour ne conserver que la moyenne, la médiane, le minimum et le maximum. Cette étape facilite le traitement ultérieur, car ces valeurs agrégées sont plus simples à manipuler que des DateTime sur toute une année. Cette approche vise à simplifier les données en les condensant autour de mesures statistiques fondamentales, ce qui facilite le traitement et l'analyse dans le contexte du modèle de machine learning.\n",
    "\n",
    "4. **Ajout de l'Employee_ID :**\n",
    "   - L'Employee_ID est ajouté pour permettre la liaison avec les autres fichiers de données.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "# Calcule les valeurs manquantes et supprime les colonnes qui ne sont pas des dates\n",
    "def clean_datetime(X,  Y):    \n",
    "    cols_to_drop = X.filter(regex='-day$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-month$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-year$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = X.columns[0]\n",
    "    X = X.drop([first_column], axis=1)\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    X = X.dropna(axis=1, how='all')\n",
    "\n",
    "    cols_to_drop = Y.filter(regex='-day$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-month$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-year$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = Y.columns[0]\n",
    "    Y = Y.drop([first_column], axis=1)\n",
    "\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    Y = Y.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "    # Calculer la médiane du temps horaire pour chaque personne\n",
    "    median_time = (Y - X).median()\n",
    "\n",
    "    for column in X.columns:\n",
    "        # Trouver où les valeurs sont manquantes\n",
    "        in_time_nan = X[column].isna()\n",
    "        out_time_nan = Y[column].isna()\n",
    "\n",
    "        # Si 'in_time' est manquant, soustraire la médiane du temps horaire à 'out_time'\n",
    "        X.loc[in_time_nan, column] = X.loc[in_time_nan, column] - median_time[column]\n",
    "\n",
    "        # Si 'out_time' est manquant, ajouter la médiane du temps horaire à 'in_time'\n",
    "        Y.loc[out_time_nan, column] = Y.loc[out_time_nan, column] + median_time[column]\n",
    "    \n",
    "    return  Y - X\n",
    "\n",
    "\n",
    "# Transformer les données pour ne garder que la moyenne, le médian, le minimum et le maximum\n",
    "class PreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].apply(lambda x: x.total_seconds() / 3600 if pd.notnull(x) else np.nan)    \n",
    "        stats = pd.DataFrame()\n",
    "        stats['mean'] = pd.Series(X.mean(axis=1))\n",
    "        stats['median'] = pd.Series(X.median(axis=1))\n",
    "        stats['min'] = pd.Series(X.min(axis=1))\n",
    "        stats['max'] = pd.Series(X.max(axis=1))\n",
    "        return stats, stats.columns\n",
    "        # Convertir les timedelta en une quantité numérique représentant le nombre d'heures\n",
    "\n",
    "\n",
    "# Définir le pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('pre_transformer', PreTransformer()),\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline\n",
    "hourly_time_prepared, stats_col = num_pipeline.fit_transform(clean_datetime(in_time, out_time))\n",
    "\n",
    "hourly_time_prepared = pd.DataFrame(hourly_time_prepared, columns= stats_col )\n",
    "\n",
    "#Ajouter la colonne 'EmployeeID' pour pouvoir joindre les DataFrames\n",
    "hourly_time_prepared['EmployeeID'] = employee_df['EmployeeID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un aperçu des données après ces transformations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_time_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointure et préparation finale des données\n",
    "\n",
    "Maintenant que chaque donnée présente dans chacun des fichiers a été explorée et qu'un prétraitement a été effectué, il est nécessaire de rassembler toutes ces données en un seul fichier qui sera utilisé ultérieurement par notre modèle.\n",
    "\n",
    "#### Fusion des données\n",
    "\n",
    "Après avoir nettoyé les fichiers de données individuellement, nous procéderons à une jointure sur l'ID employé présent dans tous les fichiers, afin de former un seul jeu de données consolidé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser la fonction merge pour faire la première jointure avec general\n",
    "result = pd.merge(employee_df, manager, on='EmployeeID')\n",
    "\n",
    "# Utiliser la fonction merge pour faire la deuxième jointure avec general_prepared\n",
    "result = pd.merge(result, general_prepared, left_on='EmployeeID', right_on='num__EmployeeID')\n",
    "\n",
    "# Utiliser la fonction merge pour faire la troisieme jointure avec hourly_time_prepared\n",
    "result = pd.merge(result, hourly_time_prepared, on='EmployeeID')\n",
    "\n",
    "# Aperçu des données\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation des données\n",
    "\n",
    "Pour préparer le jeu de données au modèle de machine learning, nous appliquons les dernières transformations. Tout d'abord, les colonnes Employee_ID qui ne sont plus utiles sont supprimées.\n",
    "\n",
    "Ensuite, une standardisation des données est appliquée afin de réduire les différences d'échelle des valeurs, assurant ainsi un poids équivalent sur toutes les colonnes. Nous avons opté pour une standardisation plutôt qu'une normalisation min-max, car elle préserve la structure originale des données.\n",
    "\n",
    "Enfin, notre jeu de données est enregistré dans un nouveau fichier CSV nommé `data_preprocessing.csv`, qui sera utilisé par la suite dans le notebook `model_training_evaluation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes Employee_ID\n",
    "result_dr = result.drop(['EmployeeID', 'num__EmployeeID'], axis=1)\n",
    "\n",
    "# Créer un pipeline\n",
    "result_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Standardiser les données\n",
    "result_array = result_transformer.fit_transform(result_dr)\n",
    "result_final = pd.DataFrame(result_array, columns = result_dr.columns)\n",
    "\n",
    "# Enregistrer les données résultantes dans un nouveau fichier CSV\n",
    "result_final.to_csv('../datasets/data_preprocessing.csv', index=False)\n",
    "\n",
    "# Aperçu du jeu de données final\n",
    "result_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélation des données\n",
    "\n",
    "Nous effectuons une analyse de corrélation des données afin de déterminer quelles colonnes ont le plus d'influence sur l'attrition. Cette étape nous permet de comprendre les relations linéaires entre différentes variables et d'identifier les facteurs qui peuvent avoir un impact significatif sur le phénomène de l'attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supposons que vous voulez prédire 'cat_nom__Attrition_Yes'\n",
    "y = result['cat_ord__Attrition']\n",
    "X = result.drop(['cat_ord__Attrition'], axis=1)\n",
    "\n",
    "# Diviser les données en un ensemble d'entraînement et un ensemble de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.DataFrame(pd.concat([X_train, y_train], axis = 1), columns=result.columns)\n",
    "\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr_matrix = train_df.corr()\n",
    "# Filtrer les corrélations\n",
    "filtered_corr_matrix = corr_matrix.where((corr_matrix >= 0.4) | (corr_matrix <= -0.4))\n",
    "\n",
    "# Trier la colonne 'cat_nom__Attrition_Yes' par valeur absolue\n",
    "sorted_corr = corr_matrix['cat_ord__Attrition'].apply(abs).sort_values(ascending=False)\n",
    "\n",
    "# Afficher les éléments les plus corrélés avec 'cat_nom__Attrition_Yes'\n",
    "sorted_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que certaines colonnes ont une influence plus prononcée sur le taux de rotation des employés. Par exemple, les dates de départ et d'arrivée, l'âge, la satisfaction au travail, ainsi que le nombre d'années de travail pour le même type de poste, semblent être des informations cruciales ayant une importance significative dans le phénomène de l'attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Dans ce livrable, nous avons entrepris une analyse approfondie et un nettoyage des données de chacun des fichiers CSV en traitant les valeurs manquantes et en numérisant les données qualitatives, entre autres. Ensuite, nous avons fusionné ces fichiers en un seul jeu de données, que nous avons standardisé pour le préparer de manière optimale au modèle de machine learning. Enfin, une fois les données prêtes, nous avons procédé à une analyse de corrélation pour identifier les paramètres ayant une influence sur l'attrition des employés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
