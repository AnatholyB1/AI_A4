{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET IA for HumanForYou\n",
    "\n",
    "|Auteur|\n",
    "|---|\n",
    "|G. DUBOYS DE LAVIGERIE|\n",
    "|T. VILETTE|\n",
    "|O. BOUSSARD|\n",
    "|A. BRICON|\n",
    "\n",
    "## Objectifs du Livrable\n",
    "\n",
    "Le présent document constitue le premier livrable du projet IA pour HumanForYou, visant à analyser et traiter les différentes données fournies par l'entreprise dans le but de comprendre les facteurs influençant le taux de rotation des employés. Les objectifs principaux de ce livrable sont les suivants :\n",
    "\n",
    "1. **Compréhension des données :** Explorer et comprendre les différentes sources de données fournies par HumanForYou, y compris les données des ressources humaines, les évaluations des managers, les enquêtes sur la qualité de vie au travail et les horaires de travail.\n",
    "\n",
    "2. **Prétraitement des données :** Nettoyer les données en éliminant les valeurs manquantes, en identifiant et en traitant les éventuelles erreurs ou incohérences, et en préparant les données pour l'analyse et la modélisation.\n",
    "\n",
    "3. **Exploration des données :** Effectuer une analyse exploratoire des données pour identifier les tendances, les relations et les motifs significatifs pouvant influencer le taux de rotation des employés.\n",
    "\n",
    "4. **Visualisation des données :** Utiliser des techniques de visualisation de données pour représenter graphiquement les principales caractéristiques des données et faciliter la compréhension des résultats.\n",
    "\n",
    "5. **Préparation des données pour la modélisation :** Préparer les données en sélectionnant les variables pertinentes, en transformant les variables catégorielles en variables numériques, et en divisant les données en ensembles d'entraînement et de test pour la modélisation.\n",
    "\n",
    "## Attendus\n",
    "\n",
    "À la fin de ce livrable, nous nous attendons à ce que les données fournies par HumanForYou soient prêtes à être analysées et utilisées pour la modélisation. Nous fournirons une analyse détaillée de la qualité des données, des tendances et des relations identifiées, ainsi qu'une documentation complète sur les étapes de prétraitement des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Préparation de l'environnement\n",
    "\n",
    "Pour faciliter l'importation des bibliothèques nécessaires au bon fonctionnement du code, veuillez exécuter le fichier `setup.ipynb`. Ce fichier s'occupe de configurer l'environnement en important toutes les librairies essentielles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibilité python 2 et python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# stabilité du notebook d'une exécution à l'autre\n",
    "np.random.seed(42)\n",
    "\n",
    "# jolies figures directement dans le notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# où sauver les figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"workflowDS\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID) # le dossier doit exister\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# ignorer les warnings inutiles (voir SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import des données\n",
    "\n",
    "Dans cette section, nous automatisons le processus d'importation des données en implémentant une fonction qui effectue les étapes suivantes :\n",
    "1. Téléchargement de l'archive contenant les fichiers.\n",
    "2. Extraction des fichiers de l'archive.\n",
    "\n",
    "Le code ci-dessous réalise le chargement des fichiers suivants :\n",
    "- `employee_survey_data.csv`\n",
    "- `general_data.csv`\n",
    "- `in_time.csv`\n",
    "- `out_time.csv`\n",
    "- `manager_survey_data.csv`\n",
    "\n",
    "De même, on va créer une fonction utilisant [`pandas`](https://pandas.pydata.org/) qui charge les données en mémoire dans un [`Pandas DataFrame`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/\"\n",
    "REPO_PATH = \"AnatholyB1/AI_A4/main/\"\n",
    "DATA_PATH = os.path.join(\"datasets\", \"all\")\n",
    "DATA_URL = DOWNLOAD_ROOT + REPO_PATH + \"data.zip\"\n",
    "\n",
    "def fetch_data(data_url=DATA_URL, data_path=DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    zip_path = os.path.join(data_path, \"data.zip\")\n",
    "    urllib.request.urlretrieve(data_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "\n",
    "fetch_data()\n",
    "\n",
    "def load_employee_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\employe\", \"employee_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_general_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\general\", \"general_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_in_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"in_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_out_time_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\in_out_time\", \"out_time.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_manager_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path + \"\\manager\", \"manager_survey_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "employee = load_employee_data()\n",
    "general = load_general_data()\n",
    "in_time = load_in_time_data()\n",
    "out_time = load_out_time_data()\n",
    "manager = load_manager_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyse des différentes données\n",
    "\n",
    "#### 3.1. employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Créez un pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "\n",
    "# Appliquez le pipeline à l'ensemble du DataFrame\n",
    "employee_transformed = numeric_transformer.fit_transform(employee)\n",
    "\n",
    "# Transformez le résultat du pipeline en DataFrame\n",
    "employee_df = pd.DataFrame(employee_transformed, columns = employee.columns)\n",
    "employee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.info()\n",
    "general.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 In_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_all_to_datetime(df):\n",
    "    year_dict = {}\n",
    "    month_dict = {}\n",
    "    day_dict = {}\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        try:\n",
    "            df[column_name] = pd.to_datetime(df[column_name])\n",
    "            year_dict[column_name + '-year'] = df[column_name].dt.year\n",
    "            month_dict[column_name + '-month'] = df[column_name].dt.month\n",
    "            day_dict[column_name + '-day'] = df[column_name].dt.day\n",
    "        except ValueError:\n",
    "            # Skip columns that cannot be converted to datetime\n",
    "            pass\n",
    "\n",
    "    year_df = pd.DataFrame(year_dict)\n",
    "    month_df = pd.DataFrame(month_dict)\n",
    "    day_df = pd.DataFrame(day_dict)\n",
    "\n",
    "    df = pd.concat([df, year_df, month_df, day_df], axis=1)\n",
    "    return df\n",
    "\n",
    "in_time = convert_all_to_datetime(in_time)\n",
    "out_time = convert_all_to_datetime(out_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suprimmer les colones qui ne sont pas des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def convert_all_to_datetime(X,  Y):    \n",
    "    cols_to_drop = X.filter(regex='-day$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-month$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = X.filter(regex='-year$').columns\n",
    "    X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = X.columns[0]\n",
    "    X = X.drop([first_column], axis=1)\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    X = X.dropna(axis=1, how='all')\n",
    "\n",
    "    cols_to_drop = Y.filter(regex='-day$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-month$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "    cols_to_drop = Y.filter(regex='-year$').columns\n",
    "    Y = Y.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    first_column = Y.columns[0]\n",
    "    Y = Y.drop([first_column], axis=1)\n",
    "\n",
    "    # Supprimer les colonnes où toutes les valeurs sont NaT\n",
    "    Y = Y.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "    # Calculer la médiane du temps horaire pour chaque personne\n",
    "    median_time = (Y - X).median()\n",
    "\n",
    "    for column in X.columns:\n",
    "        # Trouver où les valeurs sont manquantes\n",
    "        in_time_nan = X[column].isna()\n",
    "        out_time_nan = Y[column].isna()\n",
    "\n",
    "        # Si 'in_time' est manquant, soustraire la médiane du temps horaire à 'out_time'\n",
    "        X.loc[in_time_nan, column] = X.loc[in_time_nan, column] - median_time[column]\n",
    "\n",
    "        # Si 'out_time' est manquant, ajouter la médiane du temps horaire à 'in_time'\n",
    "        Y.loc[out_time_nan, column] = Y.loc[out_time_nan, column] + median_time[column]\n",
    "    \n",
    "    return  Y - X\n",
    "\n",
    "\n",
    "\n",
    "class PreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].apply(lambda x: x.total_seconds() / 3600 if pd.notnull(x) else np.nan)    \n",
    "        stats = pd.DataFrame()\n",
    "        stats['mean'] = pd.Series(X.mean(axis=1))\n",
    "        stats['median'] = pd.Series(X.median(axis=1))\n",
    "        stats['min'] = pd.Series(X.min(axis=1))\n",
    "        stats['max'] = pd.Series(X.max(axis=1))\n",
    "        print(stats.head())\n",
    "        return stats\n",
    "        # Convertir les timedelta en une quantité numérique représentant le nombre d'heures\n",
    "\n",
    "\n",
    "# Définir le pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('pre_transformer', PreTransformer()),\n",
    "    ('std_scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline\n",
    "hourly_time_prepared = num_pipeline.fit_transform(convert_all_to_datetime(in_time, out_time))\n",
    "\n",
    "hourly_time_prepared\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
